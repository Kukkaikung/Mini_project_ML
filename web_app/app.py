import os
import cv2
import easyocr
import numpy as np
from flask import Flask, render_template, request, Response, jsonify, session, redirect, url_for
from ultralytics import YOLO
from PIL import Image, ImageDraw, ImageFont
import torch
import torch.nn as nn
import base64
from torchvision import transforms

app = Flask(__name__)
app.secret_key = os.urandom(24)

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• YOLOv8
model = YOLO("C://Users//ASUS//Desktop//Mini_project_ML//model//new_yolo_patience_model.pt")

# ‡πÇ‡∏´‡∏•‡∏î EasyOCR
reader = easyocr.Reader(['en', 'th'])

# ‡πÇ‡∏´‡∏•‡∏î‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
font_path = "C://Users//ASUS//Desktop//Mini_project_ML//front//THSarabunNew Bold.ttf"
if not os.path.exists(font_path):
    print("‚ùå Thai font not found!")

# ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Ç‡∏ô‡∏≤‡∏î‡∏õ‡πâ‡∏≤‡∏¢‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
MIN_WIDTH, MIN_HEIGHT, MIN_AREA = 49, 24, 1176

# ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç characters ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
characters = "0123456789‡∏Å‡∏Ç‡∏Ñ‡∏Ü‡∏á‡∏à‡∏â‡∏ä‡∏ã‡∏ç‡∏é‡∏è‡∏ê‡∏ë‡∏í‡∏ì‡∏î‡∏ï‡∏ñ‡∏ó‡∏ò‡∏ô‡∏ö‡∏õ‡∏ú‡∏ù‡∏û‡∏ü‡∏†‡∏°‡∏¢‡∏£‡∏•‡∏ß‡∏®‡∏©‡∏™‡∏´‡∏¨‡∏≠‡∏Æ _"


plate_thumbnails = []

# ‡πÇ‡∏°‡πÄ‡∏î‡∏• CRNN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR
class CRNN(nn.Module):
    def __init__(self, num_classes):
        super(CRNN, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),
            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(inplace=True), nn.MaxPool2d(2, 2),
            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),
            nn.MaxPool2d((2, 2), (2, 1), (0, 1))
        )
        self.rnn = nn.GRU(1024, 256, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.cnn(x)
        b, c, h, w = x.size()
        x = x.permute(0, 3, 1, 2).reshape(b, w, c * h)
        x, _ = self.rnn(x)
        x = self.fc(x)
        return nn.functional.log_softmax(x, dim=2)

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• CRNN
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_crnn = CRNN(num_classes=len(characters) + 1)

try:
    model_crnn.load_state_dict(torch.load('C://Users//ASUS//Desktop//Mini_project_ML//model//new_thai_ocr_model.pth', map_location=device))
    print("‚úÖ CRNN Model loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading CRNN model: {e}")

model_crnn = model_crnn.to(device)
model_crnn.eval()

# Transform ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR
transform = transforms.Compose([
    transforms.Resize((32, 128)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
def draw_text_thai(image, text, position, font_size=32, color=(0, 255, 0)):
    font = ImageFont.truetype(font_path, font_size)
    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(image_pil)
    draw.text(position, text, font=font, fill=color)
    return cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™ CTC (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥)
def decode_predictions(preds):
    pred_str = []
    last_char = None
    for p in preds:
        if p != last_char and p < len(characters):
            pred_str.append(characters[p])
        last_char = p
    return "".join(pred_str).strip()

# ‚úÖ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ CRNN OCR
def predict(img_pil):
    print("üîç Pre-processing OCR input...")
    img = transform(img_pil).unsqueeze(0).to(device)
    print("‚úÖ Image shape:", img.shape)  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•
    
    with torch.no_grad():
        outputs = model_crnn(img)
    
    print("üìä CRNN output shape:", outputs.shape)  # ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô (1, w, num_classes)
    preds = torch.argmax(outputs, dim=2)
    preds = preds.squeeze(0).tolist()
    
    print("üìù Raw Predictions (Converted):", preds)
    
    pred_text = decode_predictions(preds)
    print(f"üîç CRNN Prediction: {pred_text}")
    return pred_text

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
def process_video(video_path, ocr_model="easyocr"):
    global plate_thumbnails 
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏¥‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÑ‡∏î‡πâ!")
        return

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = model(frame)
        license_plate_images = []
        for result in results:
            for box in result.boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                if (x2 - x1) >= MIN_WIDTH and (y2 - y1) >= MIN_HEIGHT:
                    plate_img = frame[y1:y2, x1:x2]
                    if ocr_model == "yoloocr":
                        plate_id = predict(Image.fromarray(cv2.cvtColor(plate_img, cv2.COLOR_BGR2RGB)))
                    else:
                        plate_id = " ".join(reader.readtext(plate_img, detail=0))
                    frame = draw_text_thai(frame, plate_id, (x1, y2 + 10))
                    license_plate_images.append(plate_img)
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        plate_thumbnails = [encode_image_to_base64(img) for img in license_plate_images]
        ret, buffer = cv2.imencode('.jpg', frame)
        frame_bytes = buffer.tobytes()
        yield (b'--frame\r\n' b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
    cap.release()

def encode_image_to_base64(image):
    _, buffer = cv2.imencode('.jpg', image)
    return base64.b64encode(buffer.tobytes()).decode('utf-8')


@app.route('/')
def index():
    return redirect(url_for('model_selection'))

@app.route('/model_selection', methods=['GET', 'POST'])
def model_selection():
    if request.method == 'POST':
        session['ocr_model'] = request.form['ocr_model']
        return redirect(url_for('upload_video'))
    return render_template('model_selection.html')

@app.route('/upload', methods=['GET', 'POST'])
def upload_video():
    if request.method == 'POST':
        file = request.files.get('video')
        if file and file.filename:
            os.makedirs('static/uploads', exist_ok=True)
            video_path = os.path.join("static/uploads", file.filename)
            file.save(video_path)
            return render_template('video.html', video_path=video_path, ocr_model=session.get('ocr_model', 'easyocr'))
    return render_template('index.html')

@app.route('/video_feed')
def video_feed():
    return Response(process_video(request.args.get("video_path"), session.get('ocr_model', 'easyocr')), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/get_plate_thumbnails')
def get_plate_thumbnails():
    global plate_thumbnails  
    return jsonify(plate_thumbnails)

if __name__ == '__main__':
    app.run(debug=True)
