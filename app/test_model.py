import torch
import torch.nn as nn
import cv2
import numpy as np
from torchvision import transforms  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£ import ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà


class BoundingBoxModel(nn.Module):
    def __init__(self):
        super(BoundingBoxModel, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.2),  # üî• Dropout 20%

            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.3),  # üî• Dropout 30%

            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Dropout(0.4),  # üî• Dropout 40%
        )
        self.fc = nn.Sequential(
            nn.Linear(128 * 28 * 28, 128),
            nn.ReLU(),
            nn.Dropout(0.5),  # üî• Dropout 50%
            nn.Linear(128, 4)
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô preprocess ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
def preprocess(frame):
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    input_tensor = transform(frame_rgb)
    input_tensor = input_tensor.unsqueeze(0)
    return input_tensor

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô postprocess ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
def postprocess(output):
    print(f"output.shape: {output.shape}")  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏°‡∏û‡πå‡∏°‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á output
    if output.shape[1] == 4:  # ‡∏ñ‡πâ‡∏≤ output ‡∏°‡∏µ 4 ‡∏Ñ‡πà‡∏≤ (x1, y1, x2, y2)
        bounding_box = output[0].cpu().detach().numpy()  # ‡πÄ‡∏≠‡∏≤ batch ‡πÅ‡∏£‡∏Å‡∏°‡∏≤
        print(f"Bounding Box (raw): {bounding_box}")  # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á bounding box

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ bounding box ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1
        bounding_box = np.clip(bounding_box, 0, 1)  # ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡πÑ‡∏°‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ 0 ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1
        print(f"Clipped Bounding Box: {bounding_box}")

        return bounding_box  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡∏±‡∏î bounding box
    else:
        raise ValueError(f"Unexpected output shape: {output.shape}")

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏î bounding box ‡∏ö‡∏ô‡∏†‡∏≤‡∏û
def draw_bounding_box(frame, bounding_box):
    x1, y1, x2, y2 = bounding_box
    height, width, _ = frame.shape
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ bounding box ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    print(f"Original Bounding Box (raw): {(x1, y1, x2, y2)}")
    
    # ‡∏Ñ‡∏π‡∏ì‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏Å‡∏±‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏û
    x1, y1, x2, y2 = int(x1 * width), int(y1 * height), int(x2 * width), int(y2 * height)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á
    print(f"Scaled Bounding Box: {(x1, y1)} -> {(x2, y2)}")
    
    # ‡∏ß‡∏≤‡∏î bounding box ‡∏ö‡∏ô‡∏†‡∏≤‡∏û
    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
    return frame



# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå)
model = torch.load("C:\\Users\\ASUS\\Downloads\\new_best_model.pth",map_location=torch.device('cpu') ,weights_only=False)

# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÄ‡∏ä‡πà‡∏ô GPU)
device = torch.device("cpu")
model.to(device)

# ‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î evaluation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
model.eval()

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ preprocess ‡πÅ‡∏•‡∏∞ postprocess ‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠
cap = cv2.VideoCapture("C:\\Users\\ASUS\\Desktop\\lpr1.mp4")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
    input_image = preprocess(frame)

    # ‡πÉ‡∏™‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
    with torch.no_grad():
        output = model(input_image)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    bounding_box = postprocess(output)
    result = draw_bounding_box(frame, bounding_box)  # ‡∏ß‡∏≤‡∏î bounding box ‡∏ö‡∏ô‡∏†‡∏≤‡∏û

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    cv2.imshow('Output', result)  # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å postprocess

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()